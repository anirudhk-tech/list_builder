[
    {
        "label": "json",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "json",
        "description": "json",
        "detail": "json",
        "documentation": {}
    },
    {
        "label": "APIRouter",
        "importPath": "fastapi",
        "description": "fastapi",
        "isExtraImport": true,
        "detail": "fastapi",
        "documentation": {}
    },
    {
        "label": "APIRouter",
        "importPath": "fastapi",
        "description": "fastapi",
        "isExtraImport": true,
        "detail": "fastapi",
        "documentation": {}
    },
    {
        "label": "Query",
        "importPath": "fastapi",
        "description": "fastapi",
        "isExtraImport": true,
        "detail": "fastapi",
        "documentation": {}
    },
    {
        "label": "FastAPI",
        "importPath": "fastapi",
        "description": "fastapi",
        "isExtraImport": true,
        "detail": "fastapi",
        "documentation": {}
    },
    {
        "label": "typing",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "typing",
        "description": "typing",
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Lead",
        "importPath": "app.models.lead",
        "description": "app.models.lead",
        "isExtraImport": true,
        "detail": "app.models.lead",
        "documentation": {}
    },
    {
        "label": "Lead",
        "importPath": "app.models.lead",
        "description": "app.models.lead",
        "isExtraImport": true,
        "detail": "app.models.lead",
        "documentation": {}
    },
    {
        "label": "Lead",
        "importPath": "app.models.lead",
        "description": "app.models.lead",
        "isExtraImport": true,
        "detail": "app.models.lead",
        "documentation": {}
    },
    {
        "label": "Lead",
        "importPath": "app.models.lead",
        "description": "app.models.lead",
        "isExtraImport": true,
        "detail": "app.models.lead",
        "documentation": {}
    },
    {
        "label": "scrape",
        "importPath": "app.scrapers.spiders.base",
        "description": "app.scrapers.spiders.base",
        "isExtraImport": true,
        "detail": "app.scrapers.spiders.base",
        "documentation": {}
    },
    {
        "label": "run_in_threadpool",
        "importPath": "starlette.concurrency",
        "description": "starlette.concurrency",
        "isExtraImport": true,
        "detail": "starlette.concurrency",
        "documentation": {}
    },
    {
        "label": "BaseModel",
        "importPath": "pydantic",
        "description": "pydantic",
        "isExtraImport": true,
        "detail": "pydantic",
        "documentation": {}
    },
    {
        "label": "HttpUrl",
        "importPath": "pydantic",
        "description": "pydantic",
        "isExtraImport": true,
        "detail": "pydantic",
        "documentation": {}
    },
    {
        "label": "Field",
        "importPath": "pydantic",
        "description": "pydantic",
        "isExtraImport": true,
        "detail": "pydantic",
        "documentation": {}
    },
    {
        "label": "BaseModel",
        "importPath": "pydantic",
        "description": "pydantic",
        "isExtraImport": true,
        "detail": "pydantic",
        "documentation": {}
    },
    {
        "label": "HttpUrl",
        "importPath": "pydantic",
        "description": "pydantic",
        "isExtraImport": true,
        "detail": "pydantic",
        "documentation": {}
    },
    {
        "label": "datetime",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "datetime",
        "description": "datetime",
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "datetime",
        "importPath": "datetime",
        "description": "datetime",
        "isExtraImport": true,
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "datetime",
        "importPath": "datetime",
        "description": "datetime",
        "isExtraImport": true,
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "uuid",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "uuid",
        "description": "uuid",
        "detail": "uuid",
        "documentation": {}
    },
    {
        "label": "get_logger",
        "importPath": "app.logging_config",
        "description": "app.logging_config",
        "isExtraImport": true,
        "detail": "app.logging_config",
        "documentation": {}
    },
    {
        "label": "get_logger",
        "importPath": "app.logging_config",
        "description": "app.logging_config",
        "isExtraImport": true,
        "detail": "app.logging_config",
        "documentation": {}
    },
    {
        "label": "get_logger",
        "importPath": "app.logging_config",
        "description": "app.logging_config",
        "isExtraImport": true,
        "detail": "app.logging_config",
        "documentation": {}
    },
    {
        "label": "configure_root_logger",
        "importPath": "app.logging_config",
        "description": "app.logging_config",
        "isExtraImport": true,
        "detail": "app.logging_config",
        "documentation": {}
    },
    {
        "label": "urllib.robotparser",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "urllib.robotparser",
        "description": "urllib.robotparser",
        "detail": "urllib.robotparser",
        "documentation": {}
    },
    {
        "label": "xml.etree.ElementTree",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "xml.etree.ElementTree",
        "description": "xml.etree.ElementTree",
        "detail": "xml.etree.ElementTree",
        "documentation": {}
    },
    {
        "label": "requests",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "requests",
        "description": "requests",
        "detail": "requests",
        "documentation": {}
    },
    {
        "label": "random",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "random",
        "description": "random",
        "detail": "random",
        "documentation": {}
    },
    {
        "label": "time",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "time",
        "description": "time",
        "detail": "time",
        "documentation": {}
    },
    {
        "label": "fetch_author_page",
        "importPath": "app.scrapers.spiders.indiehackers.fetch",
        "description": "app.scrapers.spiders.indiehackers.fetch",
        "isExtraImport": true,
        "detail": "app.scrapers.spiders.indiehackers.fetch",
        "documentation": {}
    },
    {
        "label": "fetch_sitemap_urls",
        "importPath": "app.scrapers.spiders.indiehackers.fetch",
        "description": "app.scrapers.spiders.indiehackers.fetch",
        "isExtraImport": true,
        "detail": "app.scrapers.spiders.indiehackers.fetch",
        "documentation": {}
    },
    {
        "label": "extract_author_urls",
        "importPath": "app.scrapers.spiders.indiehackers.utils",
        "description": "app.scrapers.spiders.indiehackers.utils",
        "isExtraImport": true,
        "detail": "app.scrapers.spiders.indiehackers.utils",
        "documentation": {}
    },
    {
        "label": "urllib.parse",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "urllib.parse",
        "description": "urllib.parse",
        "detail": "urllib.parse",
        "documentation": {}
    },
    {
        "label": "urlparse",
        "importPath": "urllib.parse",
        "description": "urllib.parse",
        "isExtraImport": true,
        "detail": "urllib.parse",
        "documentation": {}
    },
    {
        "label": "unquote",
        "importPath": "urllib.parse",
        "description": "urllib.parse",
        "isExtraImport": true,
        "detail": "urllib.parse",
        "documentation": {}
    },
    {
        "label": "bs4",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "bs4",
        "description": "bs4",
        "detail": "bs4",
        "documentation": {}
    },
    {
        "label": "re",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "re",
        "description": "re",
        "detail": "re",
        "documentation": {}
    },
    {
        "label": "scrape_indiehackers",
        "importPath": "app.scrapers.spiders.indiehackers.scrape",
        "description": "app.scrapers.spiders.indiehackers.scrape",
        "isExtraImport": true,
        "detail": "app.scrapers.spiders.indiehackers.scrape",
        "documentation": {}
    },
    {
        "label": "logging",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "logging",
        "description": "logging",
        "detail": "logging",
        "documentation": {}
    },
    {
        "label": "sys",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "sys",
        "description": "sys",
        "detail": "sys",
        "documentation": {}
    },
    {
        "label": "router",
        "importPath": "app.api.v1.endpoints.health",
        "description": "app.api.v1.endpoints.health",
        "isExtraImport": true,
        "detail": "app.api.v1.endpoints.health",
        "documentation": {}
    },
    {
        "label": "router",
        "importPath": "app.api.v1.endpoints.leads",
        "description": "app.api.v1.endpoints.leads",
        "isExtraImport": true,
        "detail": "app.api.v1.endpoints.leads",
        "documentation": {}
    },
    {
        "label": "_Known",
        "kind": 6,
        "importPath": "client.node_modules.flatted.python.flatted",
        "description": "client.node_modules.flatted.python.flatted",
        "peekOfCode": "class _Known:\n    def __init__(self):\n        self.key = []\n        self.value = []\nclass _String:\n    def __init__(self, value):\n        self.value = value\ndef _array_keys(value):\n    keys = []\n    i = 0",
        "detail": "client.node_modules.flatted.python.flatted",
        "documentation": {}
    },
    {
        "label": "_String",
        "kind": 6,
        "importPath": "client.node_modules.flatted.python.flatted",
        "description": "client.node_modules.flatted.python.flatted",
        "peekOfCode": "class _String:\n    def __init__(self, value):\n        self.value = value\ndef _array_keys(value):\n    keys = []\n    i = 0\n    for _ in value:\n        keys.append(i)\n        i += 1\n    return keys",
        "detail": "client.node_modules.flatted.python.flatted",
        "documentation": {}
    },
    {
        "label": "parse",
        "kind": 2,
        "importPath": "client.node_modules.flatted.python.flatted",
        "description": "client.node_modules.flatted.python.flatted",
        "peekOfCode": "def parse(value, *args, **kwargs):\n    json = _json.loads(value, *args, **kwargs)\n    wrapped = []\n    for value in json:\n        wrapped.append(_wrap(value))\n    input = []\n    for value in wrapped:\n        if isinstance(value, _String):\n            input.append(value.value)\n        else:",
        "detail": "client.node_modules.flatted.python.flatted",
        "documentation": {}
    },
    {
        "label": "stringify",
        "kind": 2,
        "importPath": "client.node_modules.flatted.python.flatted",
        "description": "client.node_modules.flatted.python.flatted",
        "peekOfCode": "def stringify(value, *args, **kwargs):\n    known = _Known()\n    input = []\n    output = []\n    i = int(_index(known, input, value))\n    while i < len(input):\n        output.append(_transform(known, input, input[i]))\n        i += 1\n    return _json.dumps(output, *args, **kwargs)",
        "detail": "client.node_modules.flatted.python.flatted",
        "documentation": {}
    },
    {
        "label": "router",
        "kind": 5,
        "importPath": "server.app.api.v1.endpoints.health",
        "description": "server.app.api.v1.endpoints.health",
        "peekOfCode": "router = APIRouter()\n@router.get(\"/health\", tags=[\"Health\"])\nasync def health_check():\n    \"\"\"\n    Health check endpoint to verify if the API is running.\n    Returns a simple JSON response indicating the status of the API.\n    \"\"\"\n    return {\"status\": \"ok\"}",
        "detail": "server.app.api.v1.endpoints.health",
        "documentation": {}
    },
    {
        "label": "router",
        "kind": 5,
        "importPath": "server.app.api.v1.endpoints.leads",
        "description": "server.app.api.v1.endpoints.leads",
        "peekOfCode": "router = APIRouter()\n@router.get(\"/leads\", response_model=List[Lead], tags=[\"Leads\"])\nasync def get_leads(\n    type: Optional[str] = Query(None, description=\"Filter by lead type\"),\n    limit: int = Query(30, ge=1, le=100),\n):\n    \"\"\"\n    Retrieve a list of leads with optional filtering by type and pagination.\n    - **type**: Optional filter to get leads of a specific type.\n    - **limit**: Number of leads to return (default is 30, max is 100).",
        "detail": "server.app.api.v1.endpoints.leads",
        "documentation": {}
    },
    {
        "label": "ContactInfo",
        "kind": 6,
        "importPath": "server.app.models.lead",
        "description": "server.app.models.lead",
        "peekOfCode": "class ContactInfo(BaseModel):\n    method: str                # e.g. \"email\", \"twitter\", \"linkedin\"\n    value: str                 # e.g. \"user@example.com\", \"@handle\"\nclass EngagementMetrics(BaseModel):\n    followers: Optional[int]\n    public_repos: Optional[int]      # for GitHub\n    recent_posts: Optional[int]      # posts in last 30 days\n    last_active: Optional[datetime]  # last timestamp we saw activity\nclass Lead(BaseModel):\n    id: str = Field(default_factory=lambda: str(uuid.uuid4()))",
        "detail": "server.app.models.lead",
        "documentation": {}
    },
    {
        "label": "EngagementMetrics",
        "kind": 6,
        "importPath": "server.app.models.lead",
        "description": "server.app.models.lead",
        "peekOfCode": "class EngagementMetrics(BaseModel):\n    followers: Optional[int]\n    public_repos: Optional[int]      # for GitHub\n    recent_posts: Optional[int]      # posts in last 30 days\n    last_active: Optional[datetime]  # last timestamp we saw activity\nclass Lead(BaseModel):\n    id: str = Field(default_factory=lambda: str(uuid.uuid4()))\n    name: Optional[str] = None         # e.g. \"John Doe\"\n    username: Optional[str] = None     # e.g. \"johndoe\"\n    profile_url: Optional[HttpUrl] = None",
        "detail": "server.app.models.lead",
        "documentation": {}
    },
    {
        "label": "Lead",
        "kind": 6,
        "importPath": "server.app.models.lead",
        "description": "server.app.models.lead",
        "peekOfCode": "class Lead(BaseModel):\n    id: str = Field(default_factory=lambda: str(uuid.uuid4()))\n    name: Optional[str] = None         # e.g. \"John Doe\"\n    username: Optional[str] = None     # e.g. \"johndoe\"\n    profile_url: Optional[HttpUrl] = None\n    title: Optional[str] = None           # e.g. \"Full-Stack Engineer\"\n    bio: Optional[str] = None\n    website: Optional[HttpUrl] = None     # e.g. \"https://example.com\"\n    location: Optional[str] = None\n    source: str            # “indiehackers”, “github”, etc.",
        "detail": "server.app.models.lead",
        "documentation": {}
    },
    {
        "label": "fetch_sitemap_urls",
        "kind": 2,
        "importPath": "server.app.scrapers.spiders.indiehackers.fetch",
        "description": "server.app.scrapers.spiders.indiehackers.fetch",
        "peekOfCode": "def fetch_sitemap_urls(sitemap_url: str) -> list[str]:\n    \"\"\"Fetch a sitemap file and return list of page URLs.\"\"\"\n    response = requests.get(sitemap_url) # Step 1: Fetch the sitemap\n    response.raise_for_status()\n    root = ET.fromstring(response.content) # Step 2: Parse the XML\n    ns = {\"ns\": \"http://www.sitemaps.org/schemas/sitemap/0.9\"} # Step 3: Define the namespace\n    return [u.find(\"ns:loc\", ns).text for u in root.findall(\"ns:url\", ns)] # Step 4: Extract URLs\ndef fetch_author_page(author: str) -> typing.Union[Lead, None]:\n    ALGOLIA_APP_ID  = \"N86T1R3OWZ\"\n    ALGOLIA_API_KEY = \"5140dac5e87f47346abbda1a34ee70c3\"",
        "detail": "server.app.scrapers.spiders.indiehackers.fetch",
        "documentation": {}
    },
    {
        "label": "fetch_author_page",
        "kind": 2,
        "importPath": "server.app.scrapers.spiders.indiehackers.fetch",
        "description": "server.app.scrapers.spiders.indiehackers.fetch",
        "peekOfCode": "def fetch_author_page(author: str) -> typing.Union[Lead, None]:\n    ALGOLIA_APP_ID  = \"N86T1R3OWZ\"\n    ALGOLIA_API_KEY = \"5140dac5e87f47346abbda1a34ee70c3\"\n    INDEX           = \"users\"\n    url = f\"https://{ALGOLIA_APP_ID}-dsn.algolia.net/1/indexes/{INDEX}/query\"\n    headers = {\n        \"X-Algolia-Application-Id\": ALGOLIA_APP_ID,\n        \"X-Algolia-API-Key\":       ALGOLIA_API_KEY,\n        \"Content-Type\":            \"application/json\"\n    }",
        "detail": "server.app.scrapers.spiders.indiehackers.fetch",
        "documentation": {}
    },
    {
        "label": "ROBOTS_URL",
        "kind": 5,
        "importPath": "server.app.scrapers.spiders.indiehackers.fetch",
        "description": "server.app.scrapers.spiders.indiehackers.fetch",
        "peekOfCode": "ROBOTS_URL = \"https://indiehackers.com/robots.txt\"\nrp = urllib.robotparser.RobotFileParser()\nrp.set_url(ROBOTS_URL)\nrp.read()\nlogger = get_logger(__name__)\ndef fetch_sitemap_urls(sitemap_url: str) -> list[str]:\n    \"\"\"Fetch a sitemap file and return list of page URLs.\"\"\"\n    response = requests.get(sitemap_url) # Step 1: Fetch the sitemap\n    response.raise_for_status()\n    root = ET.fromstring(response.content) # Step 2: Parse the XML",
        "detail": "server.app.scrapers.spiders.indiehackers.fetch",
        "documentation": {}
    },
    {
        "label": "rp",
        "kind": 5,
        "importPath": "server.app.scrapers.spiders.indiehackers.fetch",
        "description": "server.app.scrapers.spiders.indiehackers.fetch",
        "peekOfCode": "rp = urllib.robotparser.RobotFileParser()\nrp.set_url(ROBOTS_URL)\nrp.read()\nlogger = get_logger(__name__)\ndef fetch_sitemap_urls(sitemap_url: str) -> list[str]:\n    \"\"\"Fetch a sitemap file and return list of page URLs.\"\"\"\n    response = requests.get(sitemap_url) # Step 1: Fetch the sitemap\n    response.raise_for_status()\n    root = ET.fromstring(response.content) # Step 2: Parse the XML\n    ns = {\"ns\": \"http://www.sitemaps.org/schemas/sitemap/0.9\"} # Step 3: Define the namespace",
        "detail": "server.app.scrapers.spiders.indiehackers.fetch",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "server.app.scrapers.spiders.indiehackers.fetch",
        "description": "server.app.scrapers.spiders.indiehackers.fetch",
        "peekOfCode": "logger = get_logger(__name__)\ndef fetch_sitemap_urls(sitemap_url: str) -> list[str]:\n    \"\"\"Fetch a sitemap file and return list of page URLs.\"\"\"\n    response = requests.get(sitemap_url) # Step 1: Fetch the sitemap\n    response.raise_for_status()\n    root = ET.fromstring(response.content) # Step 2: Parse the XML\n    ns = {\"ns\": \"http://www.sitemaps.org/schemas/sitemap/0.9\"} # Step 3: Define the namespace\n    return [u.find(\"ns:loc\", ns).text for u in root.findall(\"ns:url\", ns)] # Step 4: Extract URLs\ndef fetch_author_page(author: str) -> typing.Union[Lead, None]:\n    ALGOLIA_APP_ID  = \"N86T1R3OWZ\"",
        "detail": "server.app.scrapers.spiders.indiehackers.fetch",
        "documentation": {}
    },
    {
        "label": "scrape_indiehackers",
        "kind": 2,
        "importPath": "server.app.scrapers.spiders.indiehackers.scrape",
        "description": "server.app.scrapers.spiders.indiehackers.scrape",
        "peekOfCode": "def scrape_indiehackers(limit: int) -> list[Lead]:\n    \"\"\"Main entry: scrape all author leads from Indie Hackers sitemaps.\"\"\"\n    sitemap_url = \"https://storage.googleapis.com/indie-hackers.appspot.com/sitemaps/ih-sitemap-5.xml\" # Only #5 has authors\n    leads: list[Lead] = []\n    lead_count = 0\n    page_urls = fetch_sitemap_urls(sitemap_url) # Step 1: Fetch page URLs\n    authors = extract_author_urls(page_urls) # Step 2: Extract author URLs in pages\n    logger.info(f\"Found {len(authors)} author URLs in {sitemap_url}\")\n    for author in authors: # Step 3: Parse each author page\n        if lead_count >= limit:",
        "detail": "server.app.scrapers.spiders.indiehackers.scrape",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "server.app.scrapers.spiders.indiehackers.scrape",
        "description": "server.app.scrapers.spiders.indiehackers.scrape",
        "peekOfCode": "logger = get_logger(__name__)\ndef scrape_indiehackers(limit: int) -> list[Lead]:\n    \"\"\"Main entry: scrape all author leads from Indie Hackers sitemaps.\"\"\"\n    sitemap_url = \"https://storage.googleapis.com/indie-hackers.appspot.com/sitemaps/ih-sitemap-5.xml\" # Only #5 has authors\n    leads: list[Lead] = []\n    lead_count = 0\n    page_urls = fetch_sitemap_urls(sitemap_url) # Step 1: Fetch page URLs\n    authors = extract_author_urls(page_urls) # Step 2: Extract author URLs in pages\n    logger.info(f\"Found {len(authors)} author URLs in {sitemap_url}\")\n    for author in authors: # Step 3: Parse each author page",
        "detail": "server.app.scrapers.spiders.indiehackers.scrape",
        "documentation": {}
    },
    {
        "label": "extract_author_urls",
        "kind": 2,
        "importPath": "server.app.scrapers.spiders.indiehackers.utils",
        "description": "server.app.scrapers.spiders.indiehackers.utils",
        "peekOfCode": "def extract_author_urls(urls: list[str]) -> list[dict[str, str]]:\n    \"\"\"Filter URLs -> slugs to only include Indie Hackers author profile pages.\"\"\"\n    authors = []\n    NON_USER = {\"post\", \"forum\", \"tag\", \"search\", \"pricing\", \"about\", \"terms\", \"privacy\", \"stories\", \"jobs\", \"events\", \"products\", \"podcast\", \"community\", \"resources\", \"help\", \"contact\", \"login\", \"signup\", \"ideas\",\"newest\",\"starting-up\",\"trending\",\"popular\", \"ai-coding-tools\", }\n    logger.info(f\"Extracting author URLs from {len(urls)} URLs\")\n    for u in urls: # Step 2: Filter URLs\n        p = urlparse(u)\n        netloc = p.netloc.lower().split(\":\", 1)[0]\n        if netloc not in (\"indiehackers.com\", \"www.indiehackers.com\"):\n            continue",
        "detail": "server.app.scrapers.spiders.indiehackers.utils",
        "documentation": {}
    },
    {
        "label": "get_algolia",
        "kind": 2,
        "importPath": "server.app.scrapers.spiders.indiehackers.utils",
        "description": "server.app.scrapers.spiders.indiehackers.utils",
        "peekOfCode": "def get_algolia(): # Initially used to fetch Algolia credentials\n    html   = requests.get(\"https://www.indiehackers.com\").text       \n    soup   = bs4.BeautifulSoup(html, \"html.parser\")\n    meta   = soup.find(\"meta\", {\n                 \"name\": re.compile(r\"indie-hackers/config/environment\", re.I)\n             })\n    env    = json.loads(urllib.parse.unquote(meta[\"content\"]))\n    print(env[\"algolia\"][\"applicationId\"])        # N86T1R3OWZ\n    print(env[\"algolia\"][\"searchOnlyApiKey\"])     # 5140dac5e87…\n    print(env[\"algolia\"])              # This contains index names",
        "detail": "server.app.scrapers.spiders.indiehackers.utils",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "server.app.scrapers.spiders.indiehackers.utils",
        "description": "server.app.scrapers.spiders.indiehackers.utils",
        "peekOfCode": "logger = get_logger(__name__)\ndef extract_author_urls(urls: list[str]) -> list[dict[str, str]]:\n    \"\"\"Filter URLs -> slugs to only include Indie Hackers author profile pages.\"\"\"\n    authors = []\n    NON_USER = {\"post\", \"forum\", \"tag\", \"search\", \"pricing\", \"about\", \"terms\", \"privacy\", \"stories\", \"jobs\", \"events\", \"products\", \"podcast\", \"community\", \"resources\", \"help\", \"contact\", \"login\", \"signup\", \"ideas\",\"newest\",\"starting-up\",\"trending\",\"popular\", \"ai-coding-tools\", }\n    logger.info(f\"Extracting author URLs from {len(urls)} URLs\")\n    for u in urls: # Step 2: Filter URLs\n        p = urlparse(u)\n        netloc = p.netloc.lower().split(\":\", 1)[0]\n        if netloc not in (\"indiehackers.com\", \"www.indiehackers.com\"):",
        "detail": "server.app.scrapers.spiders.indiehackers.utils",
        "documentation": {}
    },
    {
        "label": "scrape",
        "kind": 2,
        "importPath": "server.app.scrapers.spiders.base",
        "description": "server.app.scrapers.spiders.base",
        "peekOfCode": "def scrape(limit: int) -> list[Lead]:\n    \"\"\"\n    Scrapes for data across all spiders and returns a list of leads.\n    \"\"\"\n    leads = scrape_indiehackers(limit)\n    return leads",
        "detail": "server.app.scrapers.spiders.base",
        "documentation": {}
    },
    {
        "label": "configure_root_logger",
        "kind": 2,
        "importPath": "server.app.logging_config",
        "description": "server.app.logging_config",
        "peekOfCode": "def configure_root_logger():\n    if logging.root.handlers:\n        return\n    logging.basicConfig(\n        level=logging.DEBUG,           \n        format=\"%(asctime)s %(levelname)s %(name)s: %(message)s\",\n        stream=sys.stdout,\n    )\ndef get_logger(name: str) -> logging.Logger:\n    return logging.getLogger(name)",
        "detail": "server.app.logging_config",
        "documentation": {}
    },
    {
        "label": "get_logger",
        "kind": 2,
        "importPath": "server.app.logging_config",
        "description": "server.app.logging_config",
        "peekOfCode": "def get_logger(name: str) -> logging.Logger:\n    return logging.getLogger(name)",
        "detail": "server.app.logging_config",
        "documentation": {}
    },
    {
        "label": "create_app",
        "kind": 2,
        "importPath": "server.app.main",
        "description": "server.app.main",
        "peekOfCode": "def create_app() -> FastAPI:\n    configure_root_logger()\n    app = FastAPI(\n        title=\"Lead Scraper API\",\n        version=\"0.1.0\",\n        docs_url=\"/docs\",\n        openapi_url=\"/openapi.json\",\n    )\n    app.include_router(health_router, prefix=\"/api/v1\")\n    app.include_router(leads_router, prefix=\"/api/v1\")",
        "detail": "server.app.main",
        "documentation": {}
    },
    {
        "label": "app",
        "kind": 5,
        "importPath": "server.app.main",
        "description": "server.app.main",
        "peekOfCode": "app = create_app()",
        "detail": "server.app.main",
        "documentation": {}
    }
]